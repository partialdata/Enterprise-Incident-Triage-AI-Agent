import json
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, Tuple

from .config import get_settings
from .logging_utils import log_extra

logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    content: str
    prompt_tokens: int
    completion_tokens: int
    cost_usd: float


class LLMClient(ABC):
    @abstractmethod
    def generate(self, prompt: str, max_tokens: int = 512) -> LLMResponse:
        raise NotImplementedError


class MockLLMClient(LLMClient):
    """
    Mock implementation for local/offline runs.
    Produces structured JSON responses deterministically and logs usage.
    """

    def __init__(self):
        settings = get_settings()
        self.cost_per_1k_tokens = float(
            getattr(settings, "cost_per_1k_tokens", "0.0")
        )

    def generate(self, prompt: str, max_tokens: int = 512) -> LLMResponse:
        # Derive trivial token counts from prompt length for accounting
        prompt_tokens = max(1, len(prompt) // 4)
        completion_tokens = min(max_tokens, 120)
        cost_usd = ((prompt_tokens + completion_tokens) / 1000) * self.cost_per_1k_tokens

        # Mock output assumes prompt contains a JSON skeleton between markers
        content = self._synthesize_response(prompt)

        logger.info(
            "llm_generate",
            extra=log_extra(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                cost_usd=round(cost_usd, 6),
            ),
        )
        return LLMResponse(
            content=content,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            cost_usd=cost_usd,
        )

    def _synthesize_response(self, prompt: str) -> str:
        # Extract JSON template enclosed by <<JSON>> markers if present
        start = prompt.find("<<JSON>>")
        end = prompt.find("<</JSON>>")
        if start != -1 and end != -1:
            json_block = prompt[start + 8 : end].strip()
            try:
                data = json.loads(json_block)
                # Return normalized JSON
                return json.dumps(data)
            except Exception:
                pass
        # Fallback to a minimal compliant JSON string
        payload: Dict[str, Any] = {
            "summary": "Mock summary generated by offline LLM.",
            "recommended_actions": ["Investigate issue", "Update ticket"],
            "rationale": "Mock rationale from prompt signals.",
        }
        return json.dumps(payload)


def build_llm_client() -> LLMClient:
    settings = get_settings()
    provider = settings.llm_provider.lower()
    if provider == "mock":
        return MockLLMClient()
    raise ValueError(f"Unsupported LLM_PROVIDER: {provider}")
